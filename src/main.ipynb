{"cells":[{"cell_type":"markdown","id":"5c149106","metadata":{"id":"5c149106"},"source":["# 모델 돌리기"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pt9qj8687NxP","executionInfo":{"status":"ok","timestamp":1717834872010,"user_tz":-540,"elapsed":25984,"user":{"displayName":"김보근","userId":"16725844715234571619"}},"outputId":"42282c84-df38-4282-c74e-348f53c9d4d3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"id":"Pt9qj8687NxP"},{"cell_type":"code","source":["import os\n","\n","# 드라이브 내에 폴더 경로 설정\n","drive_folder_path = '/content/sample_data/image1vs1'\n","\n","# 폴더 생성\n","os.makedirs(drive_folder_path, exist_ok=True)\n","\n","%cd /content/sample_data/image1vs1\n","\n","!unzip -qq \"/content/drive/MyDrive/Colab Notebooks/EANN - (src에 코드)/Data/image1vs1.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtIeHdoOlCrM","executionInfo":{"status":"ok","timestamp":1717834882033,"user_tz":-540,"elapsed":10030,"user":{"displayName":"김보근","userId":"16725844715234571619"}},"outputId":"40f1d269-4991-4e39-ecbe-4acba636c305"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/sample_data/image1vs1\n"]}],"id":"FtIeHdoOlCrM"},{"cell_type":"code","source":["import glob\n","filepaths = glob.glob('/content/sample_data/image1vs1/*')\n","print(f\"Number of images found: {len(filepaths)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8Gtzytlme1O","executionInfo":{"status":"ok","timestamp":1717834882033,"user_tz":-540,"elapsed":8,"user":{"displayName":"김보근","userId":"16725844715234571619"}},"outputId":"110e8d83-0ba4-47d5-d2a1-c26aaa1efde6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images found: 6238\n"]}],"id":"U8Gtzytlme1O"},{"cell_type":"code","source":["pip install ipynb"],"metadata":{"id":"wGUb-RRyCyU6","executionInfo":{"status":"ok","timestamp":1717834889265,"user_tz":-540,"elapsed":7237,"user":{"displayName":"김보근","userId":"16725844715234571619"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"46090e62-34c6-4dce-eb93-04ca592e62f9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ipynb\n","  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n","Installing collected packages: ipynb\n","Successfully installed ipynb-0.5.1\n"]}],"id":"wGUb-RRyCyU6"},{"cell_type":"code","source":["pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7N-q3KZfNM_i","executionInfo":{"status":"ok","timestamp":1717834896723,"user_tz":-540,"elapsed":7464,"user":{"displayName":"김보근","userId":"16725844715234571619"}},"outputId":"46a908fc-d20a-4fbc-ebd2-6766039d2f26"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.0)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"]}],"id":"7N-q3KZfNM_i"},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/EANN - (src에 코드)/src\n","\n","import ipynb\n","import importlib\n","import process_data_coupang"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPay-kHPJP39","executionInfo":{"status":"ok","timestamp":1717834908351,"user_tz":-540,"elapsed":11657,"user":{"displayName":"김보근","userId":"16725844715234571619"}},"outputId":"7668ca0c-bafa-4ebe-fce3-ba53a4fee8d8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/EANN - (src에 코드)/src\n"]}],"id":"MPay-kHPJP39"},{"cell_type":"code","execution_count":7,"id":"e4568ace","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"e4568ace","outputId":"6bc7dcd0-5e7e-4a29-add4-068abbf9bd53","executionInfo":{"status":"ok","timestamp":1717835243338,"user_tz":-540,"elapsed":335028,"user":{"displayName":"김보근","userId":"16725844715234571619"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["loading data\n","Text and image\n","Image length 6238\n","Original post length is 3821\n","Original data frame is (3821, 14)\n","Sponsored :1867\n","Non Sponsored :1954\n","-------------------------------------\n","data size is 3821\n","paired post length is 3821\n","paried data has 16 dimension\n","Original post length is 1175\n","Original data frame is (1175, 14)\n","Sponsored :574\n","Non Sponsored :601\n","-------------------------------------\n","data size is 1175\n","paired post length is 1175\n","paried data has 16 dimension\n","Original post length is 1242\n","Original data frame is (1242, 14)\n","Sponsored :678\n","Non Sponsored :564\n","-------------------------------------\n","data size is 1242\n","paired post length is 1242\n","paried data has 16 dimension\n","loading data...\n","number of sentences: 6238\n","vocab size: 48299\n","max sentence length: 3455\n","word2vec loaded!\n","num words already in word2vec: 4626\n","translate data to embedding\n","translate test data to embedding\n","sequence length 250\n","Train Data Size is 3821\n","Finished loading data \n","TEXT: 3821, Image: 3821, label: 3821, Event: 3821\n","TEXT: 1175, Image: 1175, label: 1175, Event: 1175\n","TEXT: 1242, Image: 1242, label: 1242, Event: 1242\n","building model\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:05<00:00, 111MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["CUDA\n","loader size 60\n","training model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv2d(input, weight, bias, self.stride,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/1],  Loss: 0.5591, Class Loss: 0.5591, Train_Acc: 0.7312,  Validate_Acc: 0.8001.\n","testing model\n","Classification Acc: 0.7343, AUC-ROC: 0.8491\n","Classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.86      0.75       564\n","           1       0.84      0.63      0.72       678\n","\n","    accuracy                           0.73      1242\n","   macro avg       0.75      0.74      0.73      1242\n","weighted avg       0.76      0.73      0.73      1242\n","\n","\n","Classification confusion matrix:\n","[[484  80]\n"," [250 428]]\n","\n"]}],"source":["import numpy as np\n","import argparse\n","import time, os\n","# import random\n","import process_data_coupang as process_data\n","import copy\n","import pickle\n","from random import sample\n","import torchvision\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.optim.lr_scheduler import StepLR, MultiStepLR, ExponentialLR\n","import torch.nn as nn\n","from torch.autograd import Variable, Function\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.autograd import Function\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","from torchvision.models.vgg import VGG19_Weights\n","\n","#from logger import Logger\n","\n","from sklearn import metrics\n","from sklearn.preprocessing import label_binarize\n","import scipy.io as sio\n","\n","class Rumor_Data(Dataset):\n","    def __init__(self, dataset):\n","        self.text = torch.from_numpy(np.array(dataset['post_text']))\n","        self.image = list(dataset['image'])\n","        #self.social_context = torch.from_numpy(np.array(dataset['social_feature']))\n","        self.mask = torch.from_numpy(np.array(dataset['mask']))\n","        self.label = torch.from_numpy(np.array(dataset['label']))\n","        self.event_label = torch.from_numpy(np.array(dataset['event_label']))\n","\n","        # 추가 feature\n","        self.img_num = torch.from_numpy(np.array(dataset['img_num']))\n","        self.title1 = torch.from_numpy(np.array(dataset['title1']))\n","        self.helpfulness = torch.from_numpy(np.array(dataset['helpfulness']))\n","        self.rate = torch.from_numpy(np.array(dataset['rate']))\n","        self.top_reviewer = torch.from_numpy(np.array(dataset['top_reviewer']))\n","        self.realname_reviewer = torch.from_numpy(np.array(dataset['realname_reviewer']))\n","        self.review_num = torch.from_numpy(np.array(dataset['review_num']))\n","        self.line_breaks = torch.from_numpy(np.array(dataset['line_breaks']))\n","\n","        # 모든 추가 피처들을 하나의 어레이로 결합\n","        self.additional_features = torch.from_numpy(np.vstack([\n","            # Review Meta\n","            np.array(dataset['img_num']), np.array(dataset['title1']), np.array(dataset['helpfulness']),\n","            np.array(dataset['rate']), np.array(dataset['review_num']), np.array(dataset['line_breaks']),\n","            # Reviewer Meta\n","            np.array(dataset['top_reviewer']), np.array(dataset['realname_reviewer'])\n","        ]).astype(np.float32).T)\n","\n","        print('TEXT: %d, Image: %d, label: %d, Event: %d'\n","               % (len(self.text), len(self.image), len(self.label), len(self.event_label)))\n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, idx):\n","        # return (self.text[idx], self.image[idx], self.mask[idx]), self.label[idx], self.event_label[idx]\n","        return (self.text[idx], self.image[idx], self.additional_features[idx], self.mask[idx]), self.label[idx], self.event_label[idx]\n","\n","\n","class ReverseLayerF(Function):\n","    #staticmethod\n","    def forward(ctx, x, alpha=1.0):\n","        ctx.alpha = alpha\n","        return x.view_as(x)\n","\n","    #staticmethod\n","    def backward(ctx, grad_output):\n","        output = grad_output.neg() * ctx.alpha\n","        return output, None\n","\n","\n","def grad_reverse(x):\n","    return ReverseLayerF()(x)\n","\n","# Neural Network Model (1 hidden layer)\n","class CNN_Fusion(nn.Module):\n","    def __init__(self, args, W):\n","        super(CNN_Fusion, self).__init__()\n","        self.args = args\n","\n","        self.event_num = args.event_num\n","\n","        vocab_size = args.vocab_size\n","        emb_dim = args.embed_dim\n","\n","        C = args.class_num\n","        self.hidden_size = args.hidden_dim\n","        self.lstm_size = args.embed_dim\n","        self.social_size = 19\n","\n","        # TEXT RNN\n","        self.embed = nn.Embedding(vocab_size, emb_dim)\n","        self.embed.weight = nn.Parameter(torch.from_numpy(W))\n","        self.lstm = nn.LSTM(self.lstm_size, self.lstm_size)\n","        self.text_fc = nn.Linear(self.lstm_size, self.hidden_size)\n","        self.text_encoder = nn.Linear(emb_dim, self.hidden_size)\n","\n","        ### TEXT CNN\n","        channel_in = 1\n","        filter_num = 20\n","        window_size = [1, 2, 3, 4]\n","        self.convs = nn.ModuleList([nn.Conv2d(channel_in, filter_num, (K, emb_dim)) for K in window_size])\n","        self.fc1 = nn.Linear(len(window_size) * filter_num, self.hidden_size)\n","\n","        self.dropout = nn.Dropout(args.dropout)\n","\n","        #IMAGE\n","        #hidden_size = args.hidden_dim\n","        vgg_19 = torchvision.models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1)\n","        for param in vgg_19.parameters():\n","            param.requires_grad = False\n","        # visual model\n","        num_ftrs = vgg_19.classifier._modules['6'].out_features\n","        self.vgg = vgg_19\n","        self.image_fc1 = nn.Linear(num_ftrs,  self.hidden_size)\n","        #self.image_fc2 = nn.Linear(512, self.hidden_size)\n","        self.image_adv = nn.Linear(self.hidden_size,  int(self.hidden_size))\n","        self.image_encoder = nn.Linear(self.hidden_size, self.hidden_size)\n","\n","\n","        ### additional feature\n","        # self.social = nn.Linear(self.social_size, self.hidden_size)\n","        #self.img_num_fc = nn.Linear(1, int(self.hidden_size))\n","        self.additional_features_fc = nn.Sequential(\n","            # additional feature 열 개수 변경 시 다음 parameter 조정 필요\n","            nn.Linear(8, args.additional_dim),\n","            nn.BatchNorm1d(args.additional_dim),\n","            nn.ReLU(),\n","            nn.Dropout(args.dropout)\n","        )\n","\n","        ## ATTENTION\n","        self.attention_layer = nn.Linear(self.hidden_size, emb_dim)\n","\n","        ## Class Classifier\n","        self.class_classifier = nn.Sequential()\n","        self.class_classifier.add_module('c_fc1', nn.Linear(2 * self.hidden_size + args.additional_dim, 2))\n","        #self.class_classifier.add_module('c_bn1', nn.BatchNorm2d(100))\n","        #self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n","        #self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n","        #self.class_classifier.add_module('c_fc2', nn.Linear(self.hidden_size, 2))\n","        #self.class_classifier.add_module('c_bn2', nn.BatchNorm2d(self.hidden_size))\n","        #self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n","        #self.class_classifier.add_module('c_fc3', nn.Linear(100, 10))\n","        self.class_classifier.add_module('c_softmax', nn.Softmax(dim=1))\n","\n","        ### Catefory Classifier\n","        self.domain_classifier = nn.Sequential()\n","        self.domain_classifier.add_module('d_fc1', nn.Linear(2 * self.hidden_size + args.additional_dim, self.hidden_size))\n","        #self.domain_classifier.add_module('d_bn1', nn.BatchNorm2d(self.hidden_size))\n","        self.domain_classifier.add_module('d_relu1', nn.LeakyReLU(True))\n","        self.domain_classifier.add_module('d_fc2', nn.Linear(self.hidden_size, self.event_num))\n","        self.domain_classifier.add_module('d_softmax', nn.Softmax(dim=1))\n","\n","    def init_hidden(self, batch_size):\n","        # Before we've done anything, we dont have any hidden state.\n","        # Refer to the Pytorch documentation to see exactly\n","        # why they have this dimensionality.\n","        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n","        return (to_var(torch.zeros(1, batch_size, self.lstm_size)),\n","                to_var(torch.zeros(1, batch_size, self.lstm_size)))\n","\n","    def conv_and_pool(self, x, conv):\n","        x = F.relu(conv(x)).squeeze(3)  # (sample number,hidden_dim, length)\n","        #x = F.avg_pool1d(x, x.size(2)).squeeze(2)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","\n","        return x\n","\n","    def forward(self, text, image, additional_features, mask):\n","        ### IMAGE #####\n","        image = self.vgg(image) #[N, 512]\n","        image = F.leaky_relu(self.image_fc1(image))\n","\n","        ##########CNN##################\n","        text = self.embed(text)\n","        text = text * mask.unsqueeze(2).expand_as(text)\n","        text = text.unsqueeze(1)\n","        text = [F.leaky_relu(conv(text)).squeeze(3) for conv in self.convs]  # [(N,hidden_dim,W), ...]*len(window_size)\n","        #text = [F.avg_pool1d(i, i.size(2)).squeeze(2) for i in text]  # [(N,hidden_dim), ...]*len(window_size)\n","        text = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in text]\n","        text = torch.cat(text, 1)\n","        text = F.leaky_relu(self.fc1(text))\n","\n","        ###########Additional Feature############\n","        additional_features = additional_features.float()\n","        additional_features = self.additional_features_fc(additional_features)\n","\n","        text_image = torch.cat((text, image, additional_features), 1)\n","        #text_image = torch.cat((text, image), 1)\n","\n","        ### Fake or real\n","        class_output = self.class_classifier(text_image)\n","        ## Domain (which Category)\n","        reverse_feature = ReverseLayerF.apply(text_image)\n","        # reverse_feature = grad_reverse(text_image)\n","        # domain_output = self.domain_classifier(reverse_feature)\n","\n","        # ### Multimodal\n","        # text_reverse_feature = grad_reverse(text)\n","        # image_reverse_feature = grad_reverse(image)\n","        # text_output = self.modal_classifier(text_reverse_feature)\n","        # image_output = self.modal_classifier(image_reverse_feature\n","        return class_output # , domain_output\n","\n","def to_var(x):\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return Variable(x)\n","\n","def to_np(x):\n","    return x.data.cpu().numpy()\n","\n","def select(train, selec_indices):\n","    temp = []\n","    for i in range(len(train)):\n","        print(\"length is \"+str(len(train[i])))\n","        print(i)\n","        #print(train[i])\n","        ele = list(train[i])\n","        temp.append([ele[i] for i in selec_indices])\n","    return temp\n","\n","def make_weights_for_balanced_classes(event, nclasses = 15):\n","    count = [0] * nclasses\n","    for item in event:\n","        count[item] += 1\n","    weight_per_class = [0.] * nclasses\n","    N = float(sum(count))\n","    for i in range(nclasses):\n","        weight_per_class[i] = N/float(count[i])\n","    weight = [0] * len(event)\n","    for idx, val in enumerate(event):\n","        weight[idx] = weight_per_class[val]\n","    return weight\n","\n","\n","def main(args):\n","    print('loading data')\n","    #    dataset = DiabetesDataset(root=args.training_file)\n","    #    train_loader = DataLoader(dataset=dataset,\n","    #                              batch_size=32,\n","    #                              shuffle=True,\n","    #                              num_workers=2)\n","\n","    # MNIST Dataset\n","    train, validation, test, W = load_data(args)\n","    test_id = test['post_id']\n","\n","    #train, validation = split_train_validation(train,  1)\n","\n","    #weights = make_weights_for_balanced_classes(train[-1], 15)\n","    #weights = torch.DoubleTensor(weights)\n","    #sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n","\n","\n","    train_dataset = Rumor_Data(train)\n","\n","    validate_dataset = Rumor_Data(validation)\n","\n","    test_dataset = Rumor_Data(test)\n","\n","    # Data Loader (Input Pipeline)\n","    train_loader = DataLoader(dataset=train_dataset,\n","                              batch_size=args.batch_size,\n","                              shuffle=True)\n","\n","    validate_loader = DataLoader(dataset = validate_dataset,\n","                                 batch_size=args.batch_size,\n","                                 shuffle=False)\n","\n","    test_loader = DataLoader(dataset=test_dataset,\n","                             batch_size=args.batch_size,\n","                             shuffle=False)\n","\n","    print('building model')\n","    model = CNN_Fusion(args, W)\n","\n","    if torch.cuda.is_available():\n","        print(\"CUDA\")\n","        model.cuda()\n","\n","    # Loss and Optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, list(model.parameters())),\n","                                 lr= args.learning_rate)\n","    #optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, list(model.parameters())),\n","                                 #lr=args.learning_rate)\n","    #scheduler = StepLR(optimizer, step_size= 10, gamma= 1)\n","\n","\n","    iter_per_epoch = len(train_loader)\n","    print(\"loader size \" + str(len(train_loader)))\n","    best_validate_acc = 0.000\n","    best_test_acc = 0.000\n","    best_loss = 100\n","    best_validate_dir = ''\n","    best_list = [0,0]\n","\n","    print('training model')\n","    adversarial = True\n","    # Train the Model\n","    for epoch in range(args.num_epochs):\n","\n","        p = float(epoch) / 100\n","        #lambd = 2. / (1. + np.exp(-10. * p)) - 1\n","        lr = 0.001 / (1. + 10 * p) ** 0.75\n","\n","        optimizer.lr = lr\n","        #rgs.lambd = lambd\n","        start_time = time.time()\n","        cost_vector = []\n","        class_cost_vector = []\n","        domain_cost_vector = []\n","        acc_vector = []\n","        valid_acc_vector = []\n","        test_acc_vector = []\n","        vali_cost_vector = []\n","        test_cost_vector = []\n","\n","        for i, (train_data, train_labels, event_labels) in enumerate(train_loader):\n","            train_text, train_image, train_add, train_mask, train_labels, event_labels = \\\n","                to_var(train_data[0]), to_var(train_data[1]), to_var(train_data[2]), to_var(train_data[3]), \\\n","                to_var(train_labels), to_var(event_labels)\n","\n","            # Forward + Backward + Optimize\n","            optimizer.zero_grad()\n","\n","            class_outputs = model(train_text, train_image, train_add, train_mask)\n","\n","            ## Fake or Real loss\n","            class_loss = criterion(class_outputs, train_labels.long())\n","\n","            # Event Loss\n","            # domain_loss = criterion(domain_outputs, event_labels.long())\n","\n","            loss = class_loss\n","            loss.backward()\n","            optimizer.step()\n","            _, argmax = torch.max(class_outputs, 1)\n","\n","            cross_entropy = True\n","\n","            if True:\n","                accuracy = (train_labels == argmax.squeeze()).float().mean()\n","            else:\n","                _, labels = torch.max(train_labels, 1)\n","                accuracy = (labels.squeeze() == argmax.squeeze()).float().mean()\n","\n","            #class_cost_vector.append(class_loss.data[0])\n","            class_cost_vector.append(class_loss.item())\n","            #domain_cost_vector.append(domain_loss.data[0])\n","            # domain_cost_vector.append(domain_loss.item())\n","\n","            #cost_vector.append(loss.data[0])\n","            cost_vector.append(loss.item())\n","\n","            #acc_vector.append(accuracy.data[0])\n","            acc_vector.append(accuracy.item())\n","\n","\n","            # if i == 0:\n","            #     train_score = to_np(class_outputs.squeeze())\n","            #     train_pred = to_np(argmax.squeeze())\n","            #     train_true = to_np(train_labels.squeeze())\n","            # else:\n","            #     class_score = np.concatenate((train_score, to_np(class_outputs.squeeze())), axis=0)\n","            #     train_pred = np.concatenate((train_pred, to_np(argmax.squeeze())), axis=0)\n","            #     train_true = np.concatenate((train_true, to_np(train_labels.squeeze())), axis=0)\n","\n","\n","\n","        model.eval()\n","        validate_acc_vector_temp = []\n","        for i, (validate_data, validate_labels, event_labels) in enumerate(validate_loader):\n","            validate_text, validate_image, validate_add, validate_mask, validate_labels, event_labels = \\\n","                to_var(validate_data[0]), to_var(validate_data[1]), to_var(validate_data[2]), to_var(validate_data[3]), \\\n","                to_var(validate_labels), to_var(event_labels)\n","            validate_outputs = model(validate_text, validate_image, validate_add, validate_mask)\n","            _, validate_argmax = torch.max(validate_outputs, 1)\n","            vali_loss = criterion(validate_outputs, validate_labels.long())\n","            #domain_loss = criterion(domain_outputs, event_labels)\n","                #_, labels = torch.max(validate_labels, 1)\n","            validate_accuracy = (validate_labels == validate_argmax.squeeze()).float().mean()\n","            #vali_cost_vector.append( vali_loss.data[0])\n","            vali_cost_vector.append( vali_loss.item())\n","                #validate_accuracy = (validate_labels == validate_argmax.squeeze()).float().mean()\n","\n","            #validate_acc_vector_temp.append(validate_accuracy.data[0])\n","            validate_acc_vector_temp.append(validate_accuracy.item())\n","        validate_acc = np.mean(validate_acc_vector_temp)\n","        valid_acc_vector.append(validate_acc)\n","        model.train()\n","        print ('Epoch [%d/%d],  Loss: %.4f, Class Loss: %.4f, Train_Acc: %.4f,  Validate_Acc: %.4f.'\n","                % (\n","                epoch + 1, args.num_epochs,  np.mean(cost_vector), np.mean(class_cost_vector),\n","                    np.mean(acc_vector),   validate_acc))\n","\n","        if validate_acc > best_validate_acc:\n","            best_validate_acc = validate_acc\n","            if not os.path.exists(args.output_file):\n","                os.mkdir(args.output_file)\n","\n","            best_validate_dir = args.output_file + str(epoch + 1) + '.pkl'\n","            torch.save(model.state_dict(), best_validate_dir)\n","\n","        duration = time.time() - start_time\n","        # print ('Epoch: %d, Mean_Cost: %.4f, Duration: %.4f, Mean_Train_Acc: %.4f, Mean_Test_Acc: %.4f'\n","        # % (epoch + 1, np.mean(cost_vector), duration, np.mean(acc_vector), np.mean(test_acc_vector)))\n","        # best_validate_dir = args.output_file + 'weibo_GPU2_out.' + str(52) + '.pkl'\n","\n","\n","\n","    # Test the Model\n","    print('testing model')\n","    model = CNN_Fusion(args, W)\n","    model.load_state_dict(torch.load(best_validate_dir))\n","    #    print(torch.cuda.is_available())\n","    if torch.cuda.is_available():\n","        model.cuda()\n","    model.eval()\n","    test_score = []\n","    test_pred = []\n","    test_true = []\n","    for i, (test_data, test_labels, event_labels) in enumerate(test_loader):\n","        test_text, test_image, test_add, test_mask, test_labels = to_var(\n","            test_data[0]), to_var(test_data[1]), to_var(test_data[2]), to_var(test_data[3]), to_var(test_labels)\n","        test_outputs = model(test_text, test_image, test_add, test_mask)\n","        _, test_argmax = torch.max(test_outputs, 1)\n","        if i == 0:\n","            test_score = to_np(test_outputs.squeeze())\n","            test_pred = to_np(test_argmax.squeeze())\n","            test_true = to_np(test_labels.squeeze())\n","        else:\n","            test_score = np.concatenate((test_score, to_np(test_outputs.squeeze())), axis=0)\n","            test_pred = np.concatenate((test_pred, to_np(test_argmax.squeeze())), axis=0)\n","            test_true = np.concatenate((test_true, to_np(test_labels.squeeze())), axis=0)\n","\n","    test_accuracy = metrics.accuracy_score(test_true, test_pred)\n","    test_f1 = metrics.f1_score(test_true, test_pred, average='macro')\n","    test_precision = metrics.precision_score(test_true, test_pred, average='macro')\n","    test_recall = metrics.recall_score(test_true, test_pred, average='macro')\n","    test_score_convert = [x[1] for x in test_score]\n","    test_aucroc = metrics.roc_auc_score(test_true, test_score_convert, average='macro')\n","\n","    test_confusion_matrix = metrics.confusion_matrix(test_true, test_pred)\n","\n","    print(\"Classification Acc: %.4f, AUC-ROC: %.4f\"\n","          % (test_accuracy, test_aucroc))\n","    print(\"Classification report:\\n%s\\n\"\n","          % (metrics.classification_report(test_true, test_pred)))\n","    print(\"Classification confusion matrix:\\n%s\\n\"\n","          % (test_confusion_matrix))\n","\n","\n","def parse_arguments(parser):\n","    parser.add_argument('training_file', type=str, metavar='<training_file>', help='')\n","    #parser.add_argument('validation_file', type=str, metavar='<validation_file>', help='')\n","    parser.add_argument('testing_file', type=str, metavar='<testing_file>', help='')\n","    parser.add_argument('output_file', type=str, metavar='<output_file>', help='')\n","\n","    parse.add_argument('--static', type=bool, default=True, help='')\n","    parser.add_argument('--sequence_length', type=int, default= 250, help='')\n","    ##### parser.add_argument('--sequence_length', type=int, default= 28, help='')\n","    parser.add_argument('--class_num', type=int, default=2, help='')\n","    parser.add_argument('--hidden_dim', type=int, default = 128, help='')\n","    parser.add_argument('--embed_dim', type=int, default=32, help='')\n","    parser.add_argument('--additional_dim', type=int, default = 64, help='')\n","    parser.add_argument('--vocab_size', type=int, default=300, help='')\n","    parser.add_argument('--dropout', type=int, default=0.5, help='')\n","    parser.add_argument('--filter_num', type=int, default=5, help='')\n","    parser.add_argument('--lambd', type=int, default= 1, help='')\n","    parser.add_argument('--text_only', type=bool, default= False, help='')\n","\n","    #    parser.add_argument('--sequence_length', type = int, default = 28, help = '')\n","    #    parser.add_argument('--input_size', type = int, default = 28, help = '')\n","    #    parser.add_argument('--hidden_size', type = int, default = 128, help = '')\n","    #    parser.add_argument('--num_layers', type = int, default = 2, help = '')\n","    #    parser.add_argument('--num_classes', type = int, default = 10, help = '')\n","    parser.add_argument('--d_iter', type=int, default = 3, help='')\n","    parser.add_argument('--batch_size', type=int, default = 64, help='')\n","    parser.add_argument('--num_epochs', type=int, default = 1, help='')\n","    parser.add_argument('--learning_rate', type=float, default = 0.0005, help='')\n","    parser.add_argument('--event_num', type=int, default = 60, help='')\n","\n","\n","    #    args = parser.parse_args()\n","    return parser\n","\n","def word2vec(post, word_id_map, W):\n","    word_embedding = []\n","    mask = []\n","    #length = []\n","\n","    for sentence in post:\n","        sen_embedding = []\n","        seq_len = len(sentence) -1\n","        mask_seq = np.zeros(args.sequence_len, dtype = np.float32)\n","        mask_seq[:len(sentence)] = 1.0\n","        for i, word in enumerate(sentence):\n","            sen_embedding.append(word_id_map.get(word, 0))\n","\n","        while len(sen_embedding) < args.sequence_len:\n","            sen_embedding.append(0)\n","\n","        word_embedding.append(copy.deepcopy(sen_embedding))\n","        mask.append(copy.deepcopy(mask_seq))\n","        #length.append(seq_len)\n","    return word_embedding, mask\n","\n","def load_data(args):\n","    train, validate, test = process_data.get_data(args.text_only)\n","    #print(train[4][0])\n","    word_vector_path = '/content/drive/MyDrive/Colab Notebooks/EANN - (src에 코드)/Data/coupang/word_embedding.pickle'\n","    f = open(word_vector_path, 'rb')\n","    weight = pickle.load(f)  # W, W2, word_idx_map, vocab\n","    W, W2, word_idx_map, vocab, max_len = weight[0], weight[1], weight[2], weight[3], weight[4]\n","    args.vocab_size = len(vocab)\n","    args.sequence_len = max_len\n","    print(\"translate data to embedding\")\n","\n","    word_embedding, mask = word2vec(validate['post_text'], word_idx_map, W)\n","    validate['post_text'] = word_embedding\n","    validate['mask'] = mask\n","\n","\n","    print(\"translate test data to embedding\")\n","    word_embedding, mask = word2vec(test['post_text'], word_idx_map, W)\n","    test['post_text'] = word_embedding\n","    test['mask']=mask\n","    #test[-2]= transform(test[-2])\n","    word_embedding, mask = word2vec(train['post_text'], word_idx_map, W)\n","    train['post_text'] = word_embedding\n","    train['mask'] = mask\n","    print(\"sequence length \" + str(args.sequence_length))\n","    print(\"Train Data Size is \"+str(len(train['post_text'])))\n","    print(\"Finished loading data \")\n","    return train, validate, test, W\n","\n","def transform(event):\n","    matrix = np.zeros([len(event), max(event) + 1])\n","\n","    #print(\"Translate  shape is \" + str(matrix))\n","    for i, l in enumerate(event):\n","        matrix[i, l] = 1.00\n","    return matrix\n","\n","if __name__ == '__main__':\n","    parse = argparse.ArgumentParser()\n","    parser = parse_arguments(parse)\n","    train = '/content/drive/MyDrive/Colab Notebooks/EANN - (src에 코드)/Data/coupang/train_id.pickle'\n","    test = '/content/drive/MyDrive/Colab Notebooks/EANN - (src에 코드)/Data/coupang/test.pickle'\n","    output = '/content/drive/MyDrive/Colab Notebooks/EANN - (src에 코드)/Data/coupang/output/'\n","    args = parser.parse_args([train, test, output])\n","\n","    main(args)"]},{"cell_type":"code","execution_count":7,"id":"e453852e","metadata":{"id":"e453852e","executionInfo":{"status":"ok","timestamp":1717835243338,"user_tz":-540,"elapsed":34,"user":{"displayName":"김보근","userId":"16725844715234571619"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}